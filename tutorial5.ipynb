{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larajakl/Computational-Linguistics/blob/main/tutorial5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU0zFJIvkE8V"
      },
      "source": [
        "# Tutorial 5: Introduction to Computational Linguistics\n",
        "\n",
        "This is the fifth tutorial with practical exercises for the lecture Introduction to Computational Linguistics in the winter semester 2024. Hands-on exercises are marked with üëã ‚öí and questions are marked with ‚ùì. Remember to first **store this notebook** in your Drive or GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## **Lesson 1: Visualizing Hidden Layers**\n",
        "\n",
        "This tutorial shows how to visualize the hidden representations/embeddings for each individual hidden layer and checkpoint with the [TensorFlow Projector API](https://projector.tensorflow.org/). Remember that you set the checkpoint. If it is set to epochs, you can visualize each hidden embedding layer for each epoch.\n",
        "\n",
        "First, we need to again create and fine-tune a model. We will use the same as for Tutorial 4."
      ],
      "metadata": {
        "id": "OkbvMW9ssaKM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvXi76C_kNq9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install bertviz transformers\n",
        "!pip install bertviz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please mount your Google Drive to store the hidden embedding layers permanently so that you can load them in the [TensorFlow Projector API](https://projector.tensorflow.org/). Alternatively, you can store the files locally on your computer:\n",
        "\n",
        "```\n",
        "from google.colab import files\n",
        "files.download(\"file.tsv\")\n",
        "```\n",
        "\n",
        " Please do not only store them in the active session, since this will not allow you to load the files in the TensorFlow Projector. You need to store the files in Google Drive or locally. Alternatively but suboptimally, you can download them from the session once they have been created (see folder structure on the left in Google Colab)."
      ],
      "metadata": {
        "id": "6rVEKGDou1IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "d3bh6nJpD832"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg7NhE2jug_v"
      },
      "source": [
        "##Simple visualization with BertViz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSbp9xo7ul21"
      },
      "outputs": [],
      "source": [
        "# Load model and retrieve attention weights\n",
        "from bertviz import head_view, model_view\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
        "\n",
        "sentence_a = \"The cat sat on the mat\"\n",
        "sentence_b = \"The cat lay on the rug\"\n",
        "\n",
        "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')\n",
        "input_ids = inputs['input_ids']\n",
        "token_type_ids = inputs['token_type_ids']\n",
        "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
        "sentence_b_start = token_type_ids[0].tolist().index(1)\n",
        "input_id_list = input_ids[0].tolist() # Batch index 0\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_id_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsRGunNNu1l2"
      },
      "source": [
        "The so-called neuron view provides a nice visual explanation of attention. Once the visualization is available, click on one of the + signs next to the inputs to extend the visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko9JSHYGu2PL"
      },
      "outputs": [],
      "source": [
        "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
        "from bertviz.neuron_view import show\n",
        "\n",
        "model_type = 'bert'\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)\n",
        "show(model, model_type, tokenizer, sentence_a, sentence_b, layer=4, head=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store hidden embedding layers for visualization\n",
        "\n",
        "We need to import some additional libraries for this step.  "
      ],
      "metadata": {
        "id": "xUtElfQWxMWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import re\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb"
      ],
      "metadata": {
        "id": "AxzmuRP94ZGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load dataset**\n",
        "\n",
        "We first need to load our example dataset again, which is the same as in Tutorial 4."
      ],
      "metadata": {
        "id": "oKoyHoRP8G7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To speed up training and reduce the processing time, we will again drastically reduce the dataset. We will use the validation set for the visualiaztion for today. For your project, you need to use a separate test set.\n"
      ],
      "metadata": {
        "id": "4RBnFEYH8X6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\")\n",
        "\n",
        "\n",
        "# Just take the first 50 tokens for speed on CPU\n",
        "def truncate(example):\n",
        "    return {\n",
        "        'text': \" \".join(example['text'].split()[:100]),\n",
        "        'label': example['label']\n",
        "    }\n",
        "\n",
        "# Take 128 random examples for train and 32 validation\n",
        "small_imdb_dataset = DatasetDict(\n",
        "    train=imdb_dataset['train'].shuffle(seed=24).select(range(128)).map(truncate),\n",
        "    val=imdb_dataset['train'].shuffle(seed=24).select(range(128, 160)).map(truncate),\n",
        ")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "small_tokenized_dataset = small_imdb_dataset.map(tokenize_function, batched=True, batch_size=16)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "ObOX19fK8XGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load pre-trained model and fine-tune**\n",
        "\n",
        "Just like in Tutorial 4, we first need to fine-tune the generic model to the specific task of emotion classification. You can either store the checkpoint in your session, in Google Drive or locally.\n",
        "\n",
        "üëã ‚öí What needs to be changed in the following code to store the checkpoints in your Google Drive or locally?"
      ],
      "metadata": {
        "id": "t43m4CvD8H0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-cased', num_labels=2)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "arguments = TrainingArguments(\n",
        "    output_dir=\"sample_cl_trainer\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_steps=8,\n",
        "    num_train_epochs=5,\n",
        "    eval_strategy=\"epoch\", # run validation at the end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    seed=224\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # calculates the accuracy\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=arguments,\n",
        "    train_dataset=small_tokenized_dataset['train'],\n",
        "    eval_dataset=small_tokenized_dataset['val'], # change to test when you do your final evaluation!\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "juIuc4798MQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09219c2c-f494-40fc-fa18-31dddbb59564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "GFuO7imuADVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load model for specific checkpoint**\n",
        "\n",
        "üëã ‚öí Load one of the checkpoints (here: status of model at a specific epoch) for the fine-tuned model.  "
      ],
      "metadata": {
        "id": "OFbpI5F2AH5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code here to load one of the checkpoints for the fine-tuned model\n",
        "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\"sample_cl_trainer/checkpoint-8\")\n",
        "\n",
        "model_inputs = tokenizer(small_tokenized_dataset['val']['text'], padding=True, truncation=True, return_tensors='pt')\n",
        "outputs = fine_tuned_model(**model_inputs, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "cQ3wvpHbABlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code stores the hidden states for each layer for the checkpoint to the folder `results_vis` in your Google Drive. Change the code if you wish to store files locally. Keep in mind that already created folders are not automatically overwritten. So if you run the code again, you first need to delete the `results_vis` folder or change the name to `results_vis_1`, etc.\n",
        "\n",
        "Keep in mind that the number of layers can be set manually. For the [default pretrained model DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) the n_layers is set to 6.\n",
        "\n",
        "‚ùì How can I get the hidden embedding layers for the model at the state of another epoch (we have five in total)?\n",
        "\n",
        "‚ùì Which elements of the following code are very dataset-specific? What needs to be changed mainly for a different dataset?"
      ],
      "metadata": {
        "id": "Wos4niax276s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/results_vis\"\n",
        "path = \"results_vis\"\n",
        "layer=0\n",
        "if not os.path.exists(path):\n",
        "  os.mkdir(path)\n",
        "\n",
        "while layer in range(len(outputs['hidden_states'])):\n",
        "  if not os.path.exists(path+'/layer_' + str(layer)):\n",
        "    os.mkdir(path+'/layer_' + str(layer))\n",
        "\n",
        "  example = 0\n",
        "  tensors = []\n",
        "  labels = []\n",
        "\n",
        "  while example in range(len(outputs['hidden_states'][layer])):\n",
        "    sp_token_position = 0\n",
        "    for token in model_inputs['input_ids'][example]:\n",
        "      if token != 101:\n",
        "        sp_token_position += 1\n",
        "      else:\n",
        "        tensor = outputs['hidden_states'][layer][example][sp_token_position]\n",
        "        tensors.append(tensor)\n",
        "        break\n",
        "\n",
        "    label = [small_tokenized_dataset['val']['text'][example],str(small_tokenized_dataset['val']['label'][example])]\n",
        "    labels.append(label)\n",
        "    example +=1\n",
        "\n",
        "  writer=SummaryWriter(path+'/layer_' + str(layer))\n",
        "  writer.add_embedding(torch.stack(tensors), metadata=labels, metadata_header=['Text','Emotion'])\n",
        "\n",
        "  layer+=1\n"
      ],
      "metadata": {
        "id": "g9ZWOetJD2-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can upload metadata and tensor file into the TensorFlow Embedding Projector [API](https://projector.tensorflow.org/).\n",
        "\n",
        "üëã ‚öí Compare the visualization of Layer 1 and Layer 6. To this end, go to the [API](https://projector.tensorflow.org/), click on the option `Load` and load the two TSV-files stored in your local results_viz folder in layer_1 first.\n",
        "\n",
        "Change the setting `Color by`to `Emotion`and the visualization method to `Custom` instead of `PCA`. Take a screenshot of the visualization before you do the same for Layer 6.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eTZ6myeoI9d_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}