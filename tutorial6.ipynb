{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larajakl/Computational-Linguistics/blob/main/tutorial6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 6: Introduction to Computational Linguistics\n",
        "\n",
        "This is the fifth tutorial with practical exercises for the lecture Introduction to Computational Linguistics in the winter semester 2024. Hands-on exercises are marked with üëã ‚öí and questions are marked with ‚ùì. Remember to first **store this notebook** in your Drive or GitHub.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dYdgeFnF9dhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## **Lesson 1: Using Large Language Models in Colab**\n",
        "\n",
        "There are many ways to use Large Language Models (LLMs) in Colab and other environments. We will use the environment [Ollama](https://ollama.com/) today. Another option that provides access to a range of LLMs is [Unsloth](https://docs.unsloth.ai/).\n",
        "\n",
        "We will first install [colab-xterm](https://github.com/InfuseAI/colab-xterm) that allows us to run a terminal within a code cell on Google Colab.\n"
      ],
      "metadata": {
        "id": "n3GcvJ9I9m8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B99T0uX5N5Cd"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type and run these two lines one after another in below terminal after run the cell (%xterm) to install and run Ollama:\n",
        "* curl -fsSL https://ollama.com/install.sh | sh\n",
        "* ollama serve & ollama pull llama3.2"
      ],
      "metadata": {
        "id": "w2RKunFoBiwp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3Pw_eekOpuW"
      },
      "outputs": [],
      "source": [
        "%xterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then use [LangChain](https://python.langchain.com/docs/introduction/), a library to support the use of LLMs."
      ],
      "metadata": {
        "id": "1fkxo1E_C_VG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovlpFAQbPpYK"
      },
      "outputs": [],
      "source": [
        "!pip -qq install langchain\n",
        "!pip -qq install langchain-core\n",
        "!pip -qq install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7gKz7iUPJ6y"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import Ollama\n",
        "llm = Ollama(model = \"llama3.2\")\n",
        "llm.predict(\"what is the Meaning of life\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"The typical color of the sky is: \")"
      ],
      "metadata": {
        "id": "i7qnuV2of9kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"which model version are you?\")"
      ],
      "metadata": {
        "id": "J8dfzFiukxob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Describe quantum physics in one short sentence of no more than 12 words\")"
      ],
      "metadata": {
        "id": "j_W7v2EblBWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Explain the latest advances in large language models to me.\")"
      ],
      "metadata": {
        "id": "SUQ2cVkElNLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\")"
      ],
      "metadata": {
        "id": "1ID4TY5flXwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero-shot Prompting**"
      ],
      "metadata": {
        "id": "A_4vNQMKll6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Text: This was the best movie I've ever seen! \\n The sentiment of the text is: \")\n",
        "# Returns positive sentiment"
      ],
      "metadata": {
        "id": "Kdut2ZvBliSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Text: The director was trying too hard. \\n The sentiment of the text is: \")\n",
        "# Returns negative sentiment"
      ],
      "metadata": {
        "id": "IyDDZh_Ml0vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Few-shot Promtping**\n",
        "Now we will try few-shot prompting on sentiment analysis.\n",
        "\n",
        "üëã ‚öí Provide your own examples for the few-shot prompting on this task. Prompt the model to provide a percentage of positive/neutral/negative for each example. Store the returned result in a variable 'out' and run through the results."
      ],
      "metadata": {
        "id": "GxpVlIwlmBb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = llm.invoke()\n",
        "\n",
        "for i in out.split(\"\\n\\n\"):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "eD-AsDWvmADo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Role Prompting**\n",
        "Role or persona prompting assigns a role or persona to the model in the prompt with the assumption that the results will be more accurate. We can compare the first prompt without role to the second prompt with a role.\n",
        "\n",
        "The idea is that role prompting results in more technical benefits and drawbacks."
      ],
      "metadata": {
        "id": "CS_WKQ60tBW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out=llm.invoke(\"Explain the pros and cons of using PyTorch.\")\n",
        "\n",
        "for i in out.split(\"\\n\\n\"):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "V2q1Tl9ooUMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = llm.invoke(\"Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\")\n",
        "\n",
        "for i in out.split(\"\\n\\n\"):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "4EpmYBmkoddq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chain-of-Thought**"
      ],
      "metadata": {
        "id": "pcF_GeF3oqBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = llm.invoke(\"Who lived longer, Mozart or Elvis?\")\n",
        "for i in out.split(\"\\n\\n\"):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "iGJ5wm1Gonds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = llm.invoke(\"Who lived longer, Mozart or Elvis? Let's think through this carefully, step by step.\")\n",
        "answer = out.split(\"\\n\\n\")\n",
        "for i in answer:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "rfMkemhVo8Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self-Consistency**"
      ],
      "metadata": {
        "id": "0iWqX9fxpOeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " out = llm.invoke(\"John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Report the answer surrounded by backticks (example: 123)\")\n",
        " for i in out.split(\"\\n\\n\"):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "1t32QrsDpM4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation on a Dataset**\n",
        "\n",
        "In this part we will evaluate the performance of LLaMa on a questions-answering dataset that focuses on arithmetic tasks called [MultiArith](https://huggingface.co/datasets/ChilleD/MultiArith)."
      ],
      "metadata": {
        "id": "EZQbLc0zwwJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets==2.16.1"
      ],
      "metadata": {
        "id": "JTbtijPlqayb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyIMQ7IHFbIx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = \"ChilleD/MultiArith\"\n",
        "\n",
        "\n",
        "train_dataset = pd.DataFrame(load_dataset(dataset, split=\"train\"))[[\"question\", \"final_ans\"]]\n",
        "test_dataset = pd.DataFrame(load_dataset(dataset, split=\"test\"))[[\"question\", \"final_ans\"]]\n",
        "\n",
        "test_dataset.columns = [\"Question\", \"Gold Answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí **Arithematic Question Answering**\n",
        "\n",
        "1. Write a prompt for answering the questions in the dataset.\n",
        "2. Post process the answers if needed.\n",
        "3. Compute the accuracy of the answers given by the LLMs.\n",
        "4. Find the examples where LLMs answer differently or wrongly."
      ],
      "metadata": {
        "id": "u6wiu2IpCJ2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "GWLHdzaICuyZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}