{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larajakl/Computational-Linguistics/blob/main/bonus_exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bonus Exercises 1: Aligning Multilingual Embedding Spaces**\n",
        "\n",
        "\n",
        "\n",
        "This notebook represents the first bonus exercises for the lecture Multilingual and Crosslingual Methods and Language Resources (2024W 340168-1). For each successfully completed bonus exercise, a maximum of three points can be achieved that will be added to the points of the final exam. The tasks to be completed in the following notebook are marked with ðŸ‘‹ âš’.\n"
      ],
      "metadata": {
        "id": "H_RsHNVC57Tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this notebook, you will perform and evaluate a supervised method for aligning the embedding spaces of two languages. The examples in the notebook rely on the language pair English-German, however, feel free to change this pair to languages of your choice from the available embeddings and dictionaries (see below)."
      ],
      "metadata": {
        "id": "UADeK6Y-6Zk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "## **Preparing the Embeddings and Data**\n",
        "\n",
        "In this notebook, we will be using fastText embeddings that represents a character-based version of the word2vec skipgram method. Details on the method can be found in the [original publication](https://aclanthology.org/Q17-1010.pdf) and [this website](https://fasttext.cc/).\n",
        "\n",
        "Pretrained fastText embeddings are available in [157 languages](https://fasttext.cc/docs/en/crawl-vectors.html). The following code cell loads the fastText embeddings for English and German.\n",
        "\n",
        "ðŸ‘‹ âš’ Please change the following download command if you wish to align other languages than English and German.\n",
        "\n",
        "Before you decide on a final language pair, please make sure that:\n",
        "1.   There are pretrained embeddings for this language (see [here](https://fasttext.cc/docs/en/crawl-vectors.html))\n",
        "2.   There is a bilingual word list available (see the [MUSE GitHub](https://github.com/facebookresearch/MUSE/tree/main) section \"Ground-truth bilingual dictionaries\")\n",
        "\n",
        "If the embeddings are available, change the two-digit ISO code in `cc.en.300.vec.g` and `cc.de.300.vec.gz` to the language(s) of your choice."
      ],
      "metadata": {
        "id": "lmoyUnpZKmma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz  # English\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz  # German"
      ],
      "metadata": {
        "id": "DkTFQP9X-IU5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Embeddings\n",
        "\n",
        "As a next step we will unzip and load the embeddings. For this alignment task, we will only use the top 100,000 words for both languages to speed up the processing. This choice of only using the top 100,000 words also depends on the lenght of the available bilingual word lists."
      ],
      "metadata": {
        "id": "r9LI8R2Q9bPQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQFF13y09xgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3395d097-286a-417d-b586-215a6f088517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100000 English embeddings\n",
            "Loaded 100000 German embeddings\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "def load_fasttext_embeddings(file_path, top_n):\n",
        "    embeddings = {}\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            # Line 0 is a header line\n",
        "            if i > 0 and i <= top_n:\n",
        "              tokens = line.decode('utf-8').strip().split(' ')\n",
        "              word = tokens[0]\n",
        "              vector = np.array(tokens[1:], dtype=np.float32)\n",
        "              vector = vector / np.linalg.norm(vector)\n",
        "              embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Load the top English and German embeddings for the top 100,000 words (100000)\n",
        "# FastText sorts the embeddings by decreasing order of word frequency by default\n",
        "en_embeddings = load_fasttext_embeddings('cc.en.300.vec.gz', 100000)\n",
        "de_embeddings = load_fasttext_embeddings('cc.de.300.vec.gz', 100000)\n",
        "\n",
        "print(f\"Loaded {len(en_embeddings)} English embeddings\")\n",
        "print(f\"Loaded {len(de_embeddings)} German embeddings\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us explore the format of the downloaded and loaded embeddings."
      ],
      "metadata": {
        "id": "yhaTWGHKEr1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The loaded embeddings represent a {type(en_embeddings)} datatype.\\n')\n",
        "print(f'Each entry represents the word and the related embedding.\\n')\n",
        "print(f'We can query the word as a key and obtain the embedding, e.g. for good the embedding is {en_embeddings[\"good\"]}.\\n')\n",
        "print(f'The dimensionality of these embeddings corresponds to {len(en_embeddings[\"good\"])}.')"
      ],
      "metadata": {
        "id": "Xb5dngocEwxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and Loading the Bilingual Word List\n",
        "\n",
        "To perform this alignment, we will use a bilingual word list that is provided by the Multilingual Unsupervised and Supervised Embeddings (MUSE) project (see [here](https://github.com/facebookresearch/MUSE/tree/main) for all languages).\n",
        "\n",
        "ðŸ‘‹ âš’ Please change the following downloading command to the language pair of your choice (as long as available on MUSE).\n"
      ],
      "metadata": {
        "id": "0TBkmAvzAbyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-de.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "er7DPSxbgP4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a bilingual word list\n",
        "\n",
        "As a next step, we will create a bilingual word list from the donwloaded text file.\n",
        "\n",
        "ðŸ‘‹ âš’ Create a list of tuples `[(en_word1, de_word1), (en_word2, de_word2),...]`from the downloaded text file in the following code cell. To complete this task, please complement the provided function `load_bilingual_word_list` where it says `Your code here`.\n",
        "\n",
        "For English-German, the first ten tuples of the list look like this:\n",
        "\n",
        "```\n",
        "[('the', 'die'), ('the', 'der'), ('the', 'dem'), ('the', 'den'), ('the', 'das'), ('and', 'sowie'), ('and', 'und'), ('was', 'war'), ('was', 'wurde'), ('for', 'fÃ¼r')]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UT-WD8SmAvkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Create a list of tuples that contain word translations\n",
        "\n",
        "Parameters:\n",
        "Text file with one bilingual word pair per line\n",
        "\n",
        "Returns:\n",
        "A list of tuples that each contains one bilingual word pair\n",
        "'''\n",
        "def load_bilingual_word_list(file_path):\n",
        "    bilingual_dict = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    # Your code here\n",
        "    return bilingual_dict\n",
        "\n",
        "# Load English-German word pairs\n",
        "en_de_pairs = load_bilingual_word_list('en-de.txt')\n",
        "\n",
        "print(en_de_pairs[:10])"
      ],
      "metadata": {
        "id": "snp8ndm5gZEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the Embeddings for our Word List\n",
        "\n",
        "As a next step, we need to see which words from the word list have a vector representation in the embedding space for both languages and create a list of corresponding embeddings for both languages.\n"
      ],
      "metadata": {
        "id": "wCujQTNRLHFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "'''\n",
        "Function to create a list of word embeddings that is parallel to a bilingual list of words\n",
        "\n",
        "Parameters:\n",
        "Bilingual list of words, embeddings in the first language, embeddings in the second language\n",
        "\n",
        "Returns:\n",
        "Two numpy arrays of embeddings that correspond two the bilingual word list\n",
        "'''\n",
        "def extract_word_embeddings(bilingual_pairs, en_embeddings, de_embeddings):\n",
        "    en_vecs = []\n",
        "    de_vecs = []\n",
        "\n",
        "    for en_word, de_word in bilingual_pairs:\n",
        "        if en_word in en_embeddings and de_word in de_embeddings:\n",
        "            en_vecs.append(en_embeddings[en_word])\n",
        "            de_vecs.append(de_embeddings[de_word])\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    en_vecs = np.array(en_vecs)\n",
        "    de_vecs = np.array(de_vecs)\n",
        "\n",
        "    return en_vecs, de_vecs\n",
        "\n",
        "# Extract English and German embeddings for the bilingual lexicon\n",
        "en_vecs, de_vecs = extract_word_embeddings(en_de_pairs, en_embeddings, de_embeddings)\n",
        "\n",
        "print(f\"Extracted {en_vecs.shape[0]} aligned word vectors.\")"
      ],
      "metadata": {
        "id": "ei3DaO3mgnX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "## **Embedding Alignment**\n",
        "\n",
        "We will now use the dictionary and embeddings to align the two vector spaces. The English vector space will be aligned to the German vector space using the [Procrustes](https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem) alignment method.\n",
        "\n",
        "Given two matrices, Procrustes finds an orthogonal matrix which most closely maps one input matrix to the other. As a first step, we need to compute this orthogonal transformation matrix.  \n",
        "\n"
      ],
      "metadata": {
        "id": "fZ8nhE-4LXNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function to perform orthogonal Procrustes alignment to learn a mapping from X to Y.\n",
        "\n",
        "Parameters:\n",
        "X (numpy array): Source language word embeddings (English)\n",
        "Y (numpy array): Target language word embeddings (German)\n",
        "\n",
        "Returns:\n",
        "W (numpy array): Orthogonal transformation matrix\n",
        "\"\"\"\n",
        "def orthogonal_procrustes(X, Y):\n",
        "    X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
        "    Y = Y / np.linalg.norm(Y, axis=1, keepdims=True)\n",
        "\n",
        "    # Compute matrix product of X^T and Y\n",
        "    M = np.dot(X.T, Y)\n",
        "\n",
        "    # Perform Singular Value Decomposition (SVD) on the matrix M\n",
        "    U, _, Vt = np.linalg.svd(M)\n",
        "\n",
        "    # Compute the orthogonal transformation matrix W\n",
        "    W = np.dot(U, Vt)\n",
        "\n",
        "    return W\n",
        "\n",
        "W = orthogonal_procrustes(en_vecs, de_vecs)\n",
        "\n",
        "print(\"Orthogonal mapping matrix learned.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmA3RxaSg3-C",
        "outputId": "c62050ce-d933-4ff5-f52f-4ce1ff474aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orthogonal mapping matrix learned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a second step, the obtained matrix is used to learn an orthogonal mapping of the English vector space to approximate it to the German vector space. Here we can transform the entire vector space of 100,000 embeddings."
      ],
      "metadata": {
        "id": "YINE8wlfIbk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Apply the learned orthogonal mapping to the source language embeddings.\n",
        "\n",
        "Parameters:\n",
        "embeddings (dict): Source language embeddings (English)\n",
        "W (numpy array): Orthogonal transformation matrix\n",
        "\n",
        "Returns:\n",
        "mapped_embeddings (dict): Transformed embeddings\n",
        "\"\"\"\n",
        "def apply_mapping(embeddings, W):\n",
        "    mapped_embeddings = {}\n",
        "    for word, vec in embeddings.items():\n",
        "        mapped_vec = np.dot(vec, W)\n",
        "        # Normalize the mapped vector\n",
        "        mapped_vec = mapped_vec / np.linalg.norm(mapped_vec)\n",
        "        mapped_embeddings[word] = mapped_vec\n",
        "    return mapped_embeddings\n",
        "\n",
        "aligned_en_embeddings = apply_mapping(en_embeddings, W)\n",
        "\n",
        "print(f\"Aligned {len(aligned_en_embeddings)} English embeddings into the German space.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjogY41Vg-CC",
        "outputId": "360df3b7-b238-4b12-df64-bd09c0dc5fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aligned 100000 English embeddings into the German space.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "## **Evaluation**\n",
        "\n",
        "In this part, you will explore two different tasks for evaluating the final vector space:\n",
        "\n",
        "\n",
        "1.   Word Translation\n",
        "2.   Cross-Lingual Analogy Completion\n",
        "\n"
      ],
      "metadata": {
        "id": "Lr2QCywEBMUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Translation\n",
        "\n",
        "We will now use the bilingual word list downloaded from MUSE to evaluate the ability of our newly created aligned embedding space to translate words from English to German.\n",
        "\n",
        "A function that takes an English word as input and ouputs the nearest neighors of the German vector space is already provided for your convenience."
      ],
      "metadata": {
        "id": "nTo8Ugi-KGAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def get_nn(word, aligned_en_embeddings, de_embeddings, top_k):\n",
        "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
        "    en_vec = aligned_en_embeddings[word]\n",
        "    de_words = list(de_embeddings.keys())\n",
        "    de_vecs = np.array(list(de_embeddings.values()))\n",
        "\n",
        "    # Compute cosine similarity between the English word vector and all German word vectors\n",
        "    en_vec = en_vec / np.linalg.norm(en_vec)\n",
        "    de_vecs_norm = de_vecs / np.linalg.norm(de_vecs, axis=1, keepdims=True)\n",
        "    similarities = cosine_similarity([en_vec], de_vecs_norm).flatten()\n",
        "\n",
        "    # Get top_k most similar German words\n",
        "    nearest_idxs = similarities.argsort()[-top_k:][::-1]\n",
        "    nearest_words = [de_words[i] for i in nearest_idxs]\n",
        "\n",
        "    return nearest_words\n",
        "\n",
        "en_word = 'the'\n",
        "nearest_neighbors = get_nn(en_word, aligned_en_embeddings, de_embeddings, 5)\n",
        "print(nearest_neighbors)"
      ],
      "metadata": {
        "id": "p63UvUddLKWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Use the already downloaded bilingual word list to evaluate the ability of our aligned vector space to translate from English to German. The output of this task should be the **accuracy** calculated on **1000 words** from the word list, i.e., how many of the first 1000 English words result in five German neighbors that correspond to the German translation from the MUSE word list.\n",
        "\n",
        "Use the provided function `get_nn` to obtain the *k* nearest words in the vector space in German, given an English input word.\n"
      ],
      "metadata": {
        "id": "zagwMc7uNqmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "6pb9wd7HOUR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Lingual Analogy Completion\n",
        "\n",
        "An analogy compares two related pairs of words, e.g. *man is to woman as king is to queen*. This task can be extended to use analogies for translation, e.g. *man is to woman as Mann ist zu Frau*.\n",
        "\n",
        "\n",
        "ðŸ‘‹ âš’ Create **twenty** examples of crosslingual analogies and see whether the aligned vector space is able to correctly complete analogies across languages, e.g. positive=(queen, KÃ¶nig), negative=(king). You can use examples from the analogy text file in GitHub for this purpose.\n",
        "\n",
        "Hints:\n",
        "\n",
        "\n",
        "*   Multilingual Analogies: To create the examples, all you need is a translation of an existing analogy. You can use the already loaded bilingual word list to obtain the translations and the existing analogy list (anlogies.txt on Github) to obtain analogies.\n",
        "*   Implementation: In the code below, you only need to change the embeddings to the German embeddings for `c` and provide the function with the German embeddings."
      ],
      "metadata": {
        "id": "Xj24LdO-Omrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def norm(vec):\n",
        "  return vec / np.linalg.norm(vec)\n",
        "\n",
        "def get_target_words(embeddings, vec_a, vec_b, vec_c, top_k):\n",
        "    words = list(embeddings.keys())\n",
        "    vecs = np.array(list(embeddings.values()))\n",
        "\n",
        "    # Compute analogy based on input vectors b+c-a (woman+king-man)\n",
        "    positive = norm(vec_b+vec_c)\n",
        "    target_vec = norm(positive - vec_a)\n",
        "    vecs_norm = vecs / np.linalg.norm(vecs, axis=1, keepdims=True)\n",
        "    similarities = cosine_similarity([target_vec], vecs_norm).flatten()\n",
        "\n",
        "    # Get top_k most similar words for the retrieved result vector d\n",
        "    nearest_idxs = similarities.argsort()[-top_k:][::-1]\n",
        "    nearest_words = [words[i] for i in nearest_idxs]\n",
        "\n",
        "    return nearest_words\n",
        "\n",
        "vec_a = norm(aligned_en_embeddings[\"man\"])\n",
        "vec_b = norm(aligned_en_embeddings[\"woman\"])\n",
        "vec_c = norm(aligned_en_embeddings[\"king\"])\n",
        "\n",
        "nearest_neighbors = get_target_words(aligned_en_embeddings, vec_a, vec_b, vec_c, 5)\n",
        "print(nearest_neighbors)"
      ],
      "metadata": {
        "id": "S40JvI6iOzMA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}