{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B4tdTuDs_3oz"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larajakl/Computational-Linguistics/blob/main/tutorial4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 4: Introduction to Computational Linguistics\n",
        "\n",
        "This is the fourth tutorial with practical exercises for the lecture Introduction to Computational Linguistics in the winter semester 2024. Hands-on exercises are marked with üëã ‚öí and questions are marked with ‚ùì. Remember to first **store this notebook** in your Drive or GitHub."
      ],
      "metadata": {
        "id": "gZ9Hk2B4Sq3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lesson 1: Hugging Face Tutorial**\n",
        "\n",
        "Hugging Face is a platform that provides access to models, datasets, and metrics. It mostly provides implementations in PyTorch and TensorFlow.\n",
        "\n",
        "\n",
        "In this tutorial, we will first focus on tokenizers and models. Afterwards, we will look into fine-tuning models. For further reading and to help your project, please check the [Hugging Face Documentation and Tutorials](https://huggingface.co/docs/transformers/index).\n",
        "\n",
        "As a first step, we need to install the libraries `transformers`to load any transformer models and the library `datasets`to have access to all the Hugging Face datasets."
      ],
      "metadata": {
        "id": "Mg3rQ5egSsYZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chrmoR_USnc5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install bertviz transformers\n",
        "!pip install accelerate --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch"
      ],
      "metadata": {
        "id": "tYgBrU_B2rHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using Hugging Face, there are two very important objects to be initialized, a **tokenizer** and a **model**.\n",
        "\n",
        "\n",
        "\n",
        "*   **Tokenizer**: converts strings or text to lists of vocabularies required by the model\n",
        "*   **Model**: takes the tokenized datasets, i.e., the vocabulary ids, and can be trained to produce a prediction\n",
        "\n",
        "We will start looking into the tokenizer first."
      ],
      "metadata": {
        "id": "R1fljaUSVMSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "\n",
        "Pre-trained language models are implemented with a tokenizer that processes their input. You can either access teoknizers with the class specific to the model, i.e., DistilBERT, or use the `AutoTokenizer` class that defaults to the optimized tokenizer of the models. The alternative would be to use `DistilBertTokenizerFast` directly."
      ],
      "metadata": {
        "id": "lwZUhjEcuW10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/i\")\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "4I30_An9uZwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now input a sample sentence to the tokenizer to create a list of vocabularies with related ids and see how to access the ids the tokenizer provides.\n",
        "\n",
        "The resulting datatype is called ` BatchEncoding`, which holds the output of a pretrained batch encoding method and is derived from a Python dictionary.\n",
        "\n",
        "**‚ùì** Why did the tokenizer generate more IDs than there are words in the input sentence?"
      ],
      "metadata": {
        "id": "TPrfty_rvDn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"Hugging Face is great!\"\n",
        "tokenized_inputs = tokenizer(input_str)\n",
        "\n",
        "# Two ways to access:\n",
        "print(tokenized_inputs.input_ids)\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "id": "ysAbiGnAMt0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is a process of several steps. First the string is tokenized, which is slightly different from the tokenization of Tutorial 2, and then the tokens converted to IDs as shown below.\n",
        "\n",
        "The tokens shown represent so-called wordpieces, which is the result of a subword tokenization algorithm underlying the AutoTokenizer algorithms. The algorithm is called WordPiece and was developed by Google when pretraining BERT. The characters `##` are a WordPiece prefix to indicate wordpieces inside a word. See the following tutorial for [more information on WordPiece](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)."
      ],
      "metadata": {
        "id": "3J-XD6yNN_mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokens = tokenizer.tokenize(input_str)\n",
        "print(f\"Tokens of the input sequence: {input_tokens}\")\n",
        "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "print(f\"IDs assigned to the intput sequence: {input_ids}\")"
      ],
      "metadata": {
        "id": "qHDezw-aOgiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert existing IDs back to text, we can use the function `decode`.\n",
        "\n"
      ],
      "metadata": {
        "id": "qqLgJD9Ype6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = tokenizer.decode(input_ids)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "YZ_PXvxJpi1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí Decode the original tokenized sequence `tokenized_inputs` in the code cell below. First, find out and print which datatype this variable is. Remember that the `decode` function only takes IDs as input."
      ],
      "metadata": {
        "id": "KGZnC7B9qm7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the original variable tokenized_inputs\n",
        "print(tokenized_inputs)"
      ],
      "metadata": {
        "id": "VcnegkXyqor0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the mystery of more IDs than visible words in the input sequence can partially be answered by the tokenization and by the fact that a special token [CLS] at the beginning of a sequence and [SEP] at the end of a sequence are added for BERT-type models.\n",
        "\n",
        "Another way to reach a similar result as with `decode` can be seen in the next code cell. The difference is that with this method the special tokens are also represented."
      ],
      "metadata": {
        "id": "by0OrtRepqho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer._tokenizer.encode(input_str)\n",
        "special = inputs.tokens\n",
        "print(special)"
      ],
      "metadata": {
        "id": "SUznB96BVmTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the output of the tokenizer to a PyTorch tensor can be done easily by adding `return_tensor = pt`to the tokenization process. Compare the following output with the output of the previous code cell."
      ],
      "metadata": {
        "id": "tWaFF3eLt0PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(\"Hugging Face is great!\", return_tensors=\"pt\")\n",
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "YupZO-tbt-DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also tokenize a number of sentences at once. To ease processing, it is common to make all sentences of the same length by padding, that is, adding the `PAD' token to sequences to turn them all into sequences of the same length."
      ],
      "metadata": {
        "id": "OtpTYQpTuddt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer([\"Hugging Face is great!\",\n",
        "                         \"The quick brown fox jumps over the lazy dog.\",\n",
        "                         \"We are learning to fine-tune models.\",\n",
        "                         ],\n",
        "                         return_tensors=\"pt\",\n",
        "                         padding=True,\n",
        "                         truncation=True)\n",
        "print(model_inputs)\n",
        "print(tokenizer.pad_token, tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "aznHyJvaudlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to decode a whole set of sentences, we can use the function batch decode."
      ],
      "metadata": {
        "id": "0G70AWabvQ3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.batch_decode(model_inputs.input_ids))\n",
        "print()\n",
        "print(\"Omitting special characters when decoding:\")\n",
        "print(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "65a-vRCUvQ_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "The way models are initialized in Hugging Face is achieved with a similar code as initializing Tokenizers. There are model-specific classes or the AutoModel classes, which is preferable when comparing different models.\n",
        "\n",
        "Hugging Face automatically sets up the architecture you need for a specific task when you specify the model class, e.g. `AutoModelForSequenceClassification` respectively the model-specific `DistilBertForSequenceClassification` need to be used when you want to do sentiment analysis, question-answering, etc. For training a model on the masked language task, you need to use other classes, such as `DistilBertForMaskedLM`. More details can be found [here](https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt).\n",
        "\n",
        "\n",
        "The main three types of models are:\n",
        "\n",
        "\n",
        "*   Encoders, e.g. BERT\n",
        "*   Decoders, e.g. GPT-2\n",
        "*   Encoder-Decoders, e.g. BART or T5, which are Machine Translation (MT) models\n",
        "\n"
      ],
      "metadata": {
        "id": "DHl1IZ6Xv_mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-cased', num_labels=2)"
      ],
      "metadata": {
        "id": "SrlZsU1DxcM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get this warning because the sequence classification parameters of the model have not yet been trained.\n",
        "\n",
        "To input a string into a model requires it to be tokenized first. Then the input is very easy. The output consists of two classes since we indicated `num_labels = 2`."
      ],
      "metadata": {
        "id": "OJDH8ZR4x2f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "\n",
        "model_outputs = model(**model_inputs)\n",
        "\n",
        "print(model_inputs)\n",
        "print(model_outputs)\n",
        "\n",
        "print(\"To convert the logits outputs to probabilities, we can use softmax again:\", torch.softmax(model_outputs.logits, dim=1))"
      ],
      "metadata": {
        "id": "dfwklamOx7o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have two output classes for a binary classification task, which is only due to the fact how Hugging Face calculates the loss.\n",
        "\n",
        "We can now use the model logits to calculate the loss, i.e., go from the model predictions to the intended label and calculate the difference with any suitable loss function. We will first do this using PyTorch.\n",
        "\n",
        "For this example, we pretend that Label 1 is the correct label and pass this information to a loss function. Since this is a binary classification task, it makes more sense to use sigmoid or cross-entropy rather than softmax as a loss function."
      ],
      "metadata": {
        "id": "14VqXP-3zHIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = torch.tensor([1])\n",
        "loss = torch.nn.functional.cross_entropy(model_outputs.logits, label)\n",
        "print(loss)\n",
        "loss.backward()\n",
        "\n",
        "# You can get the parameters\n",
        "list(model.named_parameters())[0]"
      ],
      "metadata": {
        "id": "TpRVsTqEzF8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also calculate the loss directly with Hugging Face."
      ],
      "metadata": {
        "id": "d1_AKNztz5CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "\n",
        "labels = ['NEGATIVE', 'POSITIVE']\n",
        "model_inputs['labels'] = torch.tensor([1])\n",
        "\n",
        "model_outputs = model(**model_inputs)\n",
        "\n",
        "\n",
        "print(model_outputs)\n",
        "print()\n",
        "print(\"Model predictions: \", labels[model_outputs.logits.argmax()])"
      ],
      "metadata": {
        "id": "czgLomWB0CGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze in more detail what happens during training, we can visualize the attention weights and hidden states.\n",
        "\n",
        "We can use BertViz to explicitly see the change of the attention weights for each layer."
      ],
      "metadata": {
        "id": "S_X87Uks2Jes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz import model_view, head_view\n",
        "\n",
        "# We need to initialize the model with setting output attentions explicitly to True\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-cased', num_labels=2, output_attentions=True)\n",
        "# We then need to explicitly encode the tokens\n",
        "inputs = tokenizer.encode(input_str, return_tensors='pt')\n",
        "outputs = model(inputs)\n",
        "\n",
        "attention = outputs[-1]\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
        "head_view(attention, tokens)"
      ],
      "metadata": {
        "id": "d_FnE4uN1X-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is also another method to analyze hidden states and attention weights."
      ],
      "metadata": {
        "id": "lGVmD5aC2dgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"distilbert/distilbert-base-cased\", output_attentions=True, output_hidden_states=True)\n",
        "model.eval()\n",
        "\n",
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    model_output = model(**model_inputs)\n",
        "\n",
        "\n",
        "print(\"Hidden state size (per layer):  \", model_output.hidden_states[0].shape)\n",
        "print(\"Attention head size (per layer):\", model_output.attentions[0].shape)     # (layer, batch, query_word_idx, key_word_idxs)\n",
        "                                                                               # y-axis is query, x-axis is key\n",
        "print(model_output)"
      ],
      "metadata": {
        "id": "0rv995SK2nHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(model_inputs.input_ids[0])\n",
        "print(tokens)\n",
        "\n",
        "n_layers = len(model_output.attentions)\n",
        "n_heads = len(model_output.attentions[0][0])\n",
        "fig, axes = plt.subplots(6, 12)\n",
        "fig.set_size_inches(18.5*2, 10.5*2)\n",
        "for layer in range(n_layers):\n",
        "    for i in range(n_heads):\n",
        "        axes[layer, i].imshow(model_output.attentions[layer][0, i])\n",
        "        axes[layer][i].set_xticks(list(range(8))) # 8 is the number of wordpieces in our example\n",
        "        axes[layer][i].set_xticklabels(labels=tokens, rotation=\"vertical\")\n",
        "        axes[layer][i].set_yticks(list(range(8))) # 8 is the number of wordpieces in our example\n",
        "        axes[layer][i].set_yticklabels(labels=tokens)\n",
        "\n",
        "        if layer == 5:\n",
        "            axes[layer, i].set(xlabel=f\"head={i}\")\n",
        "        if i == 0:\n",
        "            axes[layer, i].set(ylabel=f\"layer={layer}\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jr4zM6px2qfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning\n",
        "\n",
        "For your final projects, you will need to finetune a pretrained language model.\n",
        "\n",
        "In addition to models, Hugging Face also provides a large repository of datasets."
      ],
      "metadata": {
        "id": "hPCP2t1A3-xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading the data**\n",
        "\n",
        "For this example we are going to work with the `imdb` dataset, which is a Large Movie Review Dataset.\n",
        "\n",
        "We will use the native PyTorch version to load the dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NkHrk0dn57r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "\n",
        "# Just take the first 50 tokens for speed on cpu\n",
        "def truncate(example):\n",
        "    return {\n",
        "        'text': \" \".join(example['text'].split()[:50]),\n",
        "        'label': example['label']\n",
        "    }\n",
        "\n",
        "# Take 128 random examples for train and 32 validation\n",
        "small_imdb_dataset = DatasetDict(\n",
        "    train=imdb_dataset['train'].shuffle(seed=24).select(range(128)).map(truncate),\n",
        "    val=imdb_dataset['train'].shuffle(seed=24).select(range(128, 160)).map(truncate),\n",
        ")"
      ],
      "metadata": {
        "id": "1xIMqRUF4STK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_imdb_dataset"
      ],
      "metadata": {
        "id": "5Tu3L15k67EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí Print the first ten examples of the training dataset of the `small_imdb_dataset`."
      ],
      "metadata": {
        "id": "e4rpftK8PQWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "62bhqrp85SfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data for use in PyTorch\n",
        "\n",
        "We need to prepare the dataset as input to the model by tokenization and padding. We also need to:\n",
        "\n",
        "1. Remove the `text` column because the model does not accept raw text as an input:\n",
        "\n",
        "    ```py\n",
        "    >>> tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "    ```\n",
        "\n",
        "2. Rename the `label` column to `labels` because the model expects the argument to be named `labels`:\n",
        "\n",
        "    ```py\n",
        "    >>> tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "    ```\n",
        "\n",
        "3. Set the format of the dataset to return PyTorch tensors instead of lists:\n",
        "\n",
        "    ```py\n",
        "    >>> tokenized_datasets.set_format(\"torch\")\n",
        "    ```"
      ],
      "metadata": {
        "id": "1mMv59IY7PRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "small_tokenized_dataset = small_imdb_dataset.map(tokenize_function, batched=True, batch_size=16)\n",
        "small_tokenized_dataset = small_tokenized_dataset.remove_columns([\"text\"])\n",
        "small_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "small_tokenized_dataset.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "4v2E6JSH7TVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now check what the first two sequences of the tokenized training dataset looks like."
      ],
      "metadata": {
        "id": "o2CZOypm7vgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_tokenized_dataset['train'][0:2]"
      ],
      "metadata": {
        "id": "kfypRDaJ7vmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create a `DataLoader` for your training and test datasets so we can iterate over batches of data:"
      ],
      "metadata": {
        "id": "ilGgIfsv8ewL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(small_tokenized_dataset['train'], batch_size=16)\n",
        "eval_dataloader = DataLoader(small_tokenized_dataset['val'], batch_size=16)"
      ],
      "metadata": {
        "id": "LdbHUcna8coc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**\n",
        "\n",
        "Hugging Face models also use `torch.nn.Module` like you did in Tutorial 3, which means backpropagation happens the same way and the same optimizers can be used. Hugging Face includes optimizers and learning rate schedules, i.e., changes along the training process, to train Transformer models.\n",
        "\n",
        "With Stochastic Gradient Descent the learning rate does not change during training. Adam represents an optimizer that extends SGD by providing an Adaptive Gradient Algorithm (AdaGrad) and an adaptation depending on the recent magnitude of gradients called Root Mean Square Propagation (RMSProp). For more details on different optimizers, see for instance [this information](https://www.ruder.io/optimizing-gradient-descent/#adam).\n"
      ],
      "metadata": {
        "id": "97g8NNEq8pxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop with Hugging Face Trainer\n",
        "\n",
        "A [Hugging Face tutorial](https://huggingface.co/docs/transformers/training) on both variants of training is available. Here we will use this `Trainer` class that covers most needs.\n",
        "\n",
        "Just to be sure tht we have the right settings, we load the dataset again and tokenize it."
      ],
      "metadata": {
        "id": "Zgns-iz8IwSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "# we had loaded the imdb dataset already above - if not, outcomment this line\n",
        "# Make sure you have the right tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\")\n",
        "\n",
        "\n",
        "# Just take the first 50 tokens for speed on CPU\n",
        "def truncate(example):\n",
        "    return {\n",
        "        'text': \" \".join(example['text'].split()[:100]),\n",
        "        'label': example['label']\n",
        "    }\n",
        "\n",
        "# Take 128 random examples for train and 32 validation\n",
        "small_imdb_dataset = DatasetDict(\n",
        "    train=imdb_dataset['train'].shuffle(seed=24).select(range(128)).map(truncate),\n",
        "    val=imdb_dataset['train'].shuffle(seed=24).select(range(128, 160)).map(truncate),\n",
        ")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "small_tokenized_dataset = small_imdb_dataset.map(tokenize_function, batched=True, batch_size=16)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "psOqoDYvJBME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can specify all the training hyperparameters by using the `TrainningArguments`class, which is detailed [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments).\n",
        "\n",
        "The `Trainer` then performs the training und you can pass all the arguments to this class, even model checkpoints to resume training later. What was the validation part in the loop above is now the `compute_metrics` function."
      ],
      "metadata": {
        "id": "Y5cD7HihKNRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-cased', num_labels=2)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "arguments = TrainingArguments(\n",
        "    output_dir=\"sample_cl_trainer\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_steps=8,\n",
        "    num_train_epochs=5,\n",
        "    eval_strategy=\"epoch\", # run validation at the end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    seed=224\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # calculates the accuracy\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=arguments,\n",
        "    train_dataset=small_tokenized_dataset['train'],\n",
        "    eval_dataset=small_tokenized_dataset['val'], # change to test when you do your final evaluation!\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "mDOF6X5LKNYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "OU0IMQaINtC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the training has been completed, evaluate the model, which is very easy with Hugging Face."
      ],
      "metadata": {
        "id": "2JxJn8y7EUWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.predict(small_tokenized_dataset['val'])\n",
        "print(results)"
      ],
      "metadata": {
        "id": "xRYZ6XXyOV9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we want to load the fine-tuned model for evaluation. We can load our models just like we load models from Hugging Face only that we need to pass the path to our models to the function `from_pretrained()`. We had 24 training steps (3 epochs times 8, where the 8 is the total number of training data (128) divided by the batch size (16)) so we can load a checkpoint for each of these steps.  "
      ],
      "metadata": {
        "id": "o-awP91-OcXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = \"I love this movie!\"\n",
        "\n",
        "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\"sample_cl_trainer/checkpoint-40\")\n",
        "model_inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
        "prediction = torch.argmax(fine_tuned_model(**model_inputs).logits)\n",
        "print([\"NEGATIVE\", \"POSITIVE\"][prediction])"
      ],
      "metadata": {
        "id": "DQ3QfFHjASxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop in PyTorch\n",
        "\n",
        "First, we will use native PyTorch for the training loop and training, since it is more transparent than the built-in Hugging Face functions.\n",
        "\n",
        "In this example we are going to use AdamW Optimizer, an extension of Adam with weight decay. And we're using a linear learning rate scheduler, which reduces the learning rate a little bit after each training step over the course of training.\n",
        "\n",
        "Let's load our model, optimizer, and learning rate scheduler. For this we need to the basic hyperparameters of the number of epochs we wish to train for and the number of training steps that depends on the size of the training dataset. We again get the same warning as before about this model not having been trained on sequence classification.\n",
        "\n",
        "To keep track of your training progress, use the [tqdm](https://tqdm.github.io/) library to add a progress bar over the number of training steps.\n",
        "\n",
        "We we also include a validation step that test the current state of the model on the validation dataset and saves a version of the model called `checkpoint`. To save the different model states as checkpoints, we first create a folder called checkpoints.  \n",
        "\n",
        "You might also want to consider early stopping, that is, interrupting the training loop when a certain threshold (value) has been reached. More information on that can be found [here](https://huggingface.co/docs/transformers/main_classes/callback#transformers.TrainerCallback)."
      ],
      "metadata": {
        "id": "B4tdTuDs_3oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-cased\", num_labels=5)\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = 3 * len(train_dataloader)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "id": "unJR1eVh-gNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last you can specify to use a `GPU`if you have access to one, e.g. on Colab, or else use a `CPU` if there is no access. Today, we will only train with `CPU`so no need to run the following cell."
      ],
      "metadata": {
        "id": "BgjoAGsQ_lI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "#model.to(device)"
      ],
      "metadata": {
        "id": "Oz0ioe-T_kxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "njZhehhSCrdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        #batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    for batch_i, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            output = model(**batch)\n",
        "        loss += output.loss\n",
        "\n",
        "    avg_val_loss = loss / len(eval_dataloader)\n",
        "    print(f\"Validation loss: {avg_val_loss}\")\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        print(\"Saving checkpoint!\")\n",
        "        best_val_loss = avg_val_loss\n",
        "        model.save_pretrained(f\"checkpoints/epoch_{epoch}.pt\")AutoTokenizer"
      ],
      "metadata": {
        "id": "TvGKMoKFAApB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}