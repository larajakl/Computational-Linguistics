{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larajakl/Computational-Linguistics/blob/main/homeexercise1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home Exericse 1: Preprocessing and NER\n",
        "In this first home exercise, you will use the knowledge from Tutorial 1 and Tutorial 2 to perform some preprocessing and NLP steps on a news article of your choice. An example article in English is provided in this notebook.\n",
        "\n",
        "In this notebook, please complete all instructions starting with 👋 ⚒ in the code cell after the sign or provide your analysis in the text cell after the sign.\n",
        "\n",
        "We will use the newspaper libabry to facilitate the scraping of the news article from a webpage."
      ],
      "metadata": {
        "id": "N4_fSCGEAFZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k"
      ],
      "metadata": {
        "id": "kxwz_cPyW4lG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee9b8c0-491d-4b13-ab4d-c8a433c7e80c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (10.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (5.1.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.8.30)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.16.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml_html_clean newspaper3k\n",
        "\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "\n",
        "url = 'https://edition.cnn.com/2024/10/25/style/banana-artwork-maurizio-cattelan-comedian-auction/index.html'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "\n",
        "#This line displays the authors of the article\n",
        "print(\"Authors: \", article.authors, \"\\n\")\n",
        "\n",
        "#This line displays the title and entire text of the article\n",
        "print(\"Title: \", article.title, \"\\n\")\n",
        "print(\"Text of article: \\n\", article.text)"
      ],
      "metadata": {
        "id": "SRiR8WeRXVOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9638a8ef-b4b7-41c1-ab90-21b0056b8850"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (5.3.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (10.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (5.1.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.8.30)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.16.1)\n",
            "Authors:  ['Oscar Holland'] \n",
            "\n",
            "Title:  Maurizio Cattelan’s viral banana artwork ‘Comedian’ could now be worth $1.5 million \n",
            "\n",
            "Text of article: \n",
            " CNN —\n",
            "\n",
            "When a banana duct-taped to a wall sold for $120,000 in 2019, social media uproar and an age-old debate about the meaning of art ensued.\n",
            "\n",
            "But artist Maurizio Cattelan’s viral creation, titled “Comedian,” may yet prove a sound investment: On Friday, auction house Sotheby’s announced that one of the artwork’s three “editions” is going back on sale — this time with an estimate of $1 million to $1.5 million.\n",
            "\n",
            "For their money, the winning bidder will receive a roll of duct tape and one banana, as well as a certificate of authenticity and official instructions for installing the work. Sotheby’s confirmed to CNN that neither the tape nor, thankfully, the banana are the originals.\n",
            "\n",
            "“‘Comedian’ is a conceptual artwork, and the actual physical materials are replaced with every installation,” an auction spokesperson said via email.\n",
            "\n",
            "Cattelan and French art gallery Perrotin made headlines around the world five years ago when they displayed “Comedian” with a six-figure asking price at the Art Basel Miami Beach fair. The original was created using a banana bought in a Miami grocery store, though the gallery said it could be replaced, as per the artist’s instructions.\n",
            "\n",
            "The art world was split on the work’s merits, though some critics saw it as rooted in the rich tradition of conceptual works — dating back to Marcel Duchamp’s famous mounted urinal — that question the value of art itself. Crowds soon formed, with fair attendees lining up to see the viral installation.\n",
            "\n",
            "Video Ad Feedback Related video: Why is art so expensive? 02:42 - Source: CNN\n",
            "\n",
            "Events took an unexpected turn when performance artist David Datuna grabbed the banana from the wall, before peeling and eating it in front of hundreds of stunned fair attendees. He later defended the move as an artistic performance in its own right, not an act of vandalism.\n",
            "\n",
            "The Miami installation was eventually removed amid public safety concerns, but all three editions were sold at the fair. Two were bought by private collectors for $120,000, while the third was purchased for a higher (but undisclosed) sum, and was later donated to The Guggenheim museum in New York.\n",
            "\n",
            "Sotheby’s would not reveal the identity of the seller in November’s auction, but said the work’s current owner had acquired it from the collection of one of the original buyers.\n",
            "\n",
            "In interviews given since the Miami installation, Cattelan has described “Comedian” as a work of commentary. Speaking to the Art Newspaper in 2021 he said it was “not a joke,” calling the viral installation “a reflection on what we value.”\n",
            "\n",
            "The Italian artist, who is known for satirical pieces that challenge popular culture, did not immediately respond to CNN’s request for comment about November’s sale.\n",
            "\n",
            "An installation shot of \"Comedian\" released by Sotheby's auction house ahead of the sale. courtesy Sotheby’s\n",
            "\n",
            "In a press release announcing the sale, Sotheby’s head of contemporary art for the Americas, David Galperin, described “Comedian” as a “defiant work of pure genius.”\n",
            "\n",
            "“Balancing profound critical thought and subversive wit, this is a defining work for the artist and for our generation,” Galperin said, adding: “If at its core, ‘Comedian’ questions the very notion of the value of art, then putting the work at auction… will be the ultimate realization of its essential conceptual idea — the public will finally have a say in deciding its true value.”\n",
            "\n",
            "While this marks the first time “Comedian” will appear at auction, the work was recently exhibited at the Leeum Museum of Art in Seoul, South Korea. It was eaten then, too: An art student from Seoul National University removed the fruit and devoured it, before taping the peel back to the wall.\n",
            "\n",
            "“The student told the museum he ate it because he was hungry,” a gallery spokesperson told CNN after the 2023 incident. The museum later replaced the eaten banana with a fresh one.\n",
            "\n",
            "Sotheby’s intends to exhibit the artwork ahead of the sale, which takes place at the auction house’s New York headquarters on November 20. “Comedian” will be put on show there on Monday before it embarks on a world tour stopping in London, Paris, Milan, Hong Kong, Dubai, Taipei, Tokyo and Los Angeles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ Use the above article or a news article of your choice and print the number of unique words in the text."
      ],
      "metadata": {
        "id": "a3xklUvEXbd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print the number of unique words in the text\n",
        "\n",
        "import re  # I import re for being able to use regular expressions\n",
        "\n",
        "text_as_list = re.split(r'[;:,\\s!.]+', article.text)  # Split the article text by any space, semicolon, colon, comma, or exclamation mark. this creates a new variable \"text_as_list\" and saves the article text as a list in this variable\n",
        "text_as_set = set(text_as_list)  # transforming the list into a set to eliminate duplicates\n",
        "\n",
        "print(text_as_set)  # prints all unique words\n",
        "print(len(text_as_set))  # prints the number of unique words\n",
        "\n",
        "# for comparison I also print these:\n",
        "print()\n",
        "print(len(text_as_list))  # prints the number of words (including duplicates)\n",
        "print(len(article.text))  # prints the number of characters in the article (because \"article.text\" is a string)\n",
        "\n",
        "# note to self: this still includes punctuation so punctuation will also be counted as words here!"
      ],
      "metadata": {
        "id": "knGLoU0kXaZb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade31bc9-6f0d-4928-84cb-3794e81d081d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'', 'artistic', 'right', 'While', 'Museum', '“‘Comedian’', 'prove', '(but', 'put', 'their', 'Milan', 'up', 'debate', 'stunned', 'three', 'originals', 'Source', 'higher', 'it', 'first', 'owner', 'would', 'rooted', 'famous', 'nor', 'so', 'had', 'and', 'vandalism', 'fruit', 'were', 'for', 'wall', 'asking', 'sum', 'critics', 'receive', 'on', 'respond', 'Perrotin', 'Tokyo', 'Kong', 'going', 'too', 'roll', 'student', 'creation', 'age-old', '-', 'New', 'could', '“Balancing', 'say', 'our', 'culture', 'contemporary', 'its', 'with', 'comment', 'winning', 'be', '20', 'store', 'because', 'place', 'request', 'using', 'eating', 'questions', 'certificate', 'Speaking', 'generation', 'Miami', 'displayed', 'act', 'tour', 'announced', 'seller', 'money', 'courtesy', 'core', 'very', 'million', 'created', 'satirical', 'incident', 'actual', 'in', 'headquarters', 'Feedback', 'marks', 'Americas', 'genius', 'Video', 'physical', 'National', 'grabbed', '“Comedian', '2019', '“not', 'Crowds', 'estimate', 'headlines', 'has', 'reveal', 'embarks', 'On', 'six-figure', 'But', 'purchased', 'lining', 'grocery', 'safety', 'some', 'notion', 'Dubai', 'buyers', 'what', 'tape', 'to', 'exhibit', 'titled', 'sound', '“If', 'front', 'email', 'turn', 'investment', 'artwork', 'social', '“Comedian”', 'Basel', 'Why', 'taping', 'ensued', 'official', 'York', \"Sotheby's\", 'intends', 'about', 'media', 'defining', 'release', 'expensive?', 'subversive', 'CNN’s', 'thankfully', 'November', 'since', 'reflection', 'urinal', '42', '“a', 'did', 'later', 'The', 'ate', '—', 'of', 'eventually', 'critical', 'all', 'this', 'realization', 'price', 'dating', 'artist’s', 'neither', 'but', 'there', 'commentary', 'rich', 'editions', 'by', 'hungry', '“The', 'joke', 'art', 'London', 'concerns', 'Two', 'duct', 'pure', 'after', 'viral', 'Events', 'works', 'spokesperson', 'He', 'profound', 'deciding', 'fresh', 'Paris', 'peel', 'recently', 'mounted', 'around', 'collectors', 'he', 'donated', 'popular', '\"Comedian\"', '$120', 'pieces', 'installation', 'uproar', 'auction', 'split', 'adding', 'fair', 'calling', 'current', 'artwork’s', 'interviews', '“defiant', 'shot', 'is', 'released', 'Seoul', '“editions”', 'see', 'Beach', 'bought', 'known', 'Duchamp’s', '$1', 'When', 'Cattelan’s', 'Guggenheim', 'took', 'via', 'was', 'attendees', 'yet', 'the', 'Friday', 'Cattelan', 'saw', 'given', 'auction…', 'world', 'unexpected', 'merits', 'video', 'CNN', 'years', 'third', 'every', 'duct-taped', 'It', 'Related', '2023', 'banana', 'ultimate', 'question', 'Newspaper', 'Monday', 'sale', 'Ad', 'French', 'hundreds', 'November’s', 'then', 'well', 'essential', 'true', 'Leeum', 'authenticity', 'work', 'which', 'own', 'not', 'exhibited', 'itself', 'finally', 'press', 'stopping', 'In', 'have', 'collection', 'head', 'described', 'For', 'installing', 'Angeles', 'soon', 'that', 'five', 'an', 'though', 'private', 'move', 'house’s', 'museum', 'formed', 'putting', 'at', 'Korea', 'artist', 'before', 'tradition', 'Galperin', 'wit', '2021', 'said', 'one', 'defended', 'performance', 'time', 'University', 'may', 'gallery', 'Hong', 'acquired', 'public', 'An', 'thought', 'house', 'they', 'undisclosed)', 'original', 'per', 'value', 'South', 'told', 'identity', 'who', 'devoured', 'show', 'will', 'bidder', 'made', 'announcing', 'immediately', 'instructions', 'meaning', 'ago', 'while', '‘Comedian’', 'idea', 'Los', 'challenge', 'as', '02', 'a', 'takes', 'back', 'Sotheby’s', 'Taipei', 'peeling', 'Italian', 'appear', 'eaten', '”', 'Datuna', 'work’s', 'Art', '5', '000', 'sold', 'conceptual', 'Marcel', 'removed', 'we', 'are', 'materials', 'confirmed', 'David', 'from', 'replaced', 'when', 'Maurizio', 'amid', 'ahead'}\n",
            "374\n",
            "\n",
            "707\n",
            "4170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**"
      ],
      "metadata": {
        "id": "2DMEt4uoXgFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ Now perform the following preprocessing steps and see how the number of unique words changes:\n",
        "\n",
        "1. Lowercase all words in the text.\n",
        "2. Remove punctuation markers and numbers (Hint: `string.isalpha()).\n",
        "3. Lemmatize all words in the text."
      ],
      "metadata": {
        "id": "pZjKWTTZX3m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text with all three steps and then calculate the number of\n",
        "# unique words in the text again\n",
        "\n",
        "# In the following, I used the NLTK lemmatizer. This was before I read in the forum that we are supposed to use the spaCy lemmatizer. See the next\n",
        "# code block for the implementation using spaCy.\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "# Lemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize lemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Here I make all tokens in the article text lowercase and save them in a list called \"text_as_list\":\n",
        "text_as_list = re.split(r'[;:,\\s!.]+', article.text.lower())\n",
        "\n",
        "# Here I initiate a new list:\n",
        "new_text_nltk = []\n",
        "\n",
        "# Here I fill the new list with (only alphabetic!) lemmatised words (I remove punctuation and numbers):\n",
        "for word in text_as_list:  # this loops through the tokens in the list\n",
        "  if word.isalpha():  # this line makes sure only alphabetic tokens (no punctuation, no numbers) are added to the new list\n",
        "    lemmatized_word = lemmatizer.lemmatize(word)  # this lemmatises the current token\n",
        "    new_text_nltk.append(lemmatized_word)  # this adds the current lemmatised toked to the new list\n",
        "  else:\n",
        "    continue  # if a word is not alphabetic, it is basically skipped\n",
        "\n",
        "# To get unique words, I transfer this new list into a set again:\n",
        "unique_words_after_preprocessing_nltk = set(new_text_nltk)\n",
        "print(unique_words_after_preprocessing_nltk) # here I print the unique words\n",
        "print(len(unique_words_after_preprocessing_nltk)) # here I print the number of unique words\n"
      ],
      "metadata": {
        "id": "-p7kAp-ZYNz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0031a37f-0010-4247-9880-05a720d3e86b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'artistic', 'right', 'prove', 'crowd', 'source', 'put', 'their', 'speaking', 'up', 'debate', 'stunned', 'three', 'higher', 'piece', 'event', 'it', 'first', 'owner', 'would', 'rooted', 'famous', 'nor', 'so', 'had', 'and', 'mark', 'vandalism', 'fruit', 'were', 'for', 'wall', 'asking', 'sum', 'cattelan', 'receive', 'on', 'respond', 'going', 'newspaper', 'too', 'roll', 'student', 'creation', 'university', 'could', 'say', 'our', 'culture', 'contemporary', 'comment', 'with', 'critic', 'winning', 'be', 'store', 'because', 'place', 'request', 'using', 'eating', 'certificate', 'generation', 'displayed', 'act', 'tour', 'announced', 'seller', 'money', 'basel', 'courtesy', 'core', 'very', 'million', 'medium', 'london', 'david', 'created', 'satirical', 'incident', 'actual', 'in', 'headquarters', 'wa', 'feedback', 'seoul', 'genius', 'physical', 'grabbed', 'estimate', 'reveal', 'embarks', 'two', 'purchased', 'lining', 'grocery', 'safety', 'concern', 'some', 'notion', 'tokyo', 'what', 'tape', 'to', 'exhibit', 'titled', 'sound', 'attendee', 'front', 'headline', 'email', 'turn', 'investment', 'artwork', 'social', 'new', 'buyer', 'dubai', 'paris', 'taping', 'america', 'merit', 'ensued', 'official', 'leeum', 'intends', 'about', 'november', 'defining', 'release', 'subversive', 'thankfully', 'since', 'reflection', 'urinal', 'did', 'later', 'ate', 'of', 'eventually', 'critical', 'all', 'angeles', 'this', 'realization', 'los', 'price', 'korea', 'but', 'dating', 'neither', 'there', 'commentary', 'rich', 'by', 'take', 'hungry', 'taipei', 'milan', 'instruction', 'joke', 'art', 'duct', 'pure', 'after', 'viral', 'spokesperson', 'profound', 'deciding', 'fresh', 'peel', 'recently', 'mounted', 'around', 'galperin', 'he', 'donated', 'popular', 'ad', 'installation', 'uproar', 'auction', 'split', 'adding', 'fair', 'calling', 'south', 'current', 'datuna', 'ha', 'italian', 'shot', 'is', 'released', 'year', 'french', 'see', 'bought', 'known', 'beach', 'took', 'via', 'perrotin', 'yet', 'the', 'saw', 'given', 'world', 'unexpected', 'video', 'third', 'every', 'banana', 'ultimate', 'question', 'sale', 'then', 'well', 'monday', 'essential', 'true', 'national', 'authenticity', 'work', 'which', 'own', 'not', 'exhibited', 'itself', 'finally', 'press', 'stopping', 'hundred', 'have', 'collection', 'maurizio', 'head', 'described', 'installing', 'interview', 'soon', 'that', 'five', 'an', 'though', 'private', 'marcel', 'move', 'related', 'museum', 'hong', 'formed', 'guggenheim', 'putting', 'friday', 'at', 'artist', 'before', 'tradition', 'wit', 'said', 'one', 'defended', 'performance', 'why', 'time', 'may', 'gallery', 'acquired', 'public', 'thought', 'house', 'they', 'original', 'per', 'value', 'told', 'material', 'identity', 'who', 'devoured', 'show', 'will', 'bidder', 'made', 'kong', 'announcing', 'immediately', 'meaning', 'collector', 'ago', 'edition', 'while', 'idea', 'challenge', 'a', 'back', 'york', 'peeling', 'appear', 'eaten', 'sold', 'conceptual', 'removed', 'we', 'are', 'confirmed', 'from', 'replaced', 'when', 'amid', 'miami', 'cnn', 'ahead'}\n",
            "313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I use spaCy for the task:\n",
        "\n",
        "# first I need to import spaCy\n",
        "import spacy\n",
        "\n",
        "# then I load the language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# then I have the text processed; specifically the text in all lowercase. This line processes the text and I tokens, lemmas, POS will be accessible.\n",
        "doc = nlp(article.text.lower())\n",
        "\n",
        "# Here I initiate a new list:\n",
        "new_text_spaCy = []\n",
        "\n",
        "# Here I fill the new list with (only alphabetic!) lemmatised words (I remove punctuation and numbers):\n",
        "for token in doc:  # this loops through the tokens in the article\n",
        "  if token.is_alpha:  # this line makes sure only alphabetic tokens (no punctuation, no numbers) are added to the new list\n",
        "    new_text_spaCy.append(token.lemma_)  # this adds the current lemmatised toked to the new list\n",
        "  else:\n",
        "    continue  # if a word is not alphabetic, it is basically skipped\n",
        "\n",
        "# To get unique words, I transfer this new list into a set again:\n",
        "unique_words_after_preprocessing_spaCy = set(new_text_spaCy)\n",
        "print(unique_words_after_preprocessing_spaCy) # here I print the unique words\n",
        "print(len(unique_words_after_preprocessing_spaCy)) # here I print the number of unique words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6O0RV6ngKIM",
        "outputId": "e3564900-c6c8-432c-948d-dbf3b5fa56b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'figure', 'artistic', 'right', 'prove', 'crowd', 'source', 'put', 'their', 'up', 'debate', 'stunned', 'three', 'piece', 'event', 'it', 'first', 'owner', 'would', 'intend', 'famous', 'nor', 'so', 'and', 'mark', 'vandalism', 'fruit', 'for', 'wall', 'grab', 'sum', 'cattelan', 'describe', 'receive', 'on', 'respond', 'newspaper', 'too', 'roll', 'student', 'creation', 'university', 'could', 'say', 'our', 'culture', 'contemporary', 'comment', 'with', 'its', 'critic', 'be', 'store', 'because', 'place', 'request', 'remove', 'certificate', 'generation', 'decide', 'act', 'undisclosed', 'tour', 'seller', 'money', 'use', 'basel', 'courtesy', 'replace', 'core', 'very', 'acquire', 'million', 'medium', 'london', 'david', 'title', 'satirical', 'incident', 'actual', 'in', 'headquarters', 'define', 'feedback', 'seoul', 'genius', 'physical', 'estimate', 'reveal', 'two', 'concern', 'grocery', 'safety', 'some', 'notion', 'add', 'tokyo', 'what', 'defend', 'tape', 'to', 'exhibit', 'sound', 'instal', 'attendee', 'front', 'headline', 'email', 'turn', 'investment', 'artwork', 'social', 'new', 'buyer', 'dubai', 'paris', 'give', 'do', 'america', 'merit', 'official', 'leeum', 'about', 'november', 'release', 'subversive', 'thankfully', 'since', 'reflection', 'urinal', 'later', 'of', 'eventually', 'critical', 'all', 'angeles', 'this', 'realization', 'los', 'korea', 'price', 'high', 'but', 'there', 'neither', 'commentary', 'rich', 'comedian', 'duchamp', 'by', 'take', 'hungry', 'taipei', 'milan', 'instruction', 'joke', 'art', 'buy', 'duct', 'balance', 'pure', 'after', 'call', 'viral', 'spokesperson', 'mount', 'go', 'devour', 'profound', 'eat', 'fresh', 'peel', 'recently', 'around', 'galperin', 'he', 'popular', 'ad', 'installation', 'uproar', 'auction', 'split', 'fair', 'south', 'current', 'datuna', 'italian', 'speak', 'shot', 'year', 'french', 'see', 'embark', 'ask', 'defiant', 'form', 'beach', 'via', 'perrotin', 'yet', 'the', 'confirm', 'world', 'unexpected', 'video', 'third', 'every', 'banana', 'ultimate', 'question', 'sale', 'create', 'stop', 'then', 'well', 'monday', 'essential', 'announce', 'true', 'national', 'authenticity', 'work', 'which', 'win', 'own', 'not', 'itself', 'finally', 'ensue', 'press', 'line', 'hundred', 'old', 'tell', 'expensive', 'have', 'collection', 'maurizio', 'head', 'interview', 'soon', 'sell', 'that', 'five', 'an', 'though', 'private', 'marcel', 'move', 'display', 'museum', 'hong', 'guggenheim', 'friday', 'at', 'artist', 'before', 'tradition', 'six', 'wit', 'one', 'performance', 'why', 'time', 'purchase', 'may', 'gallery', 'date', 'age', 'public', 'thought', 'house', 'they', 'original', 'per', 'value', 'material', 'identity', 'know', 'who', 'show', 'will', 'bidder', 'kong', 'make', 'immediately', 'meaning', 'collector', 'ago', 'root', 'edition', 'while', 'idea', 'challenge', 'as', 'a', 'back', 'york', 'appear', 'if', 'eaten', 'donate', 'conceptual', 'we', 'from', 'when', 'amid', 'relate', 'miami', 'sotheby', 'cnn', 'ahead'}\n",
            "311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NER**\n",
        "\n",
        "In the tutorial we have only used one of the different models available in spaCy. In this exercise, you will compare the performance of the different models of different sizes and implementations. A description of the type of available models is in the [spaCy documentation](https://spacy.io/models/en). First, the models to be used need to be installed. We will use the following three models."
      ],
      "metadata": {
        "id": "F81suSVkaS1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "2JRubivyaSBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2fc89fc-6969-45a4-ae6e-25b21b6b6e83"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-trf==3.7.3\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.3/en_core_web_trf-3.7.3-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-trf==3.7.3) (3.7.5)\n",
            "Requirement already satisfied: spacy-curated-transformers<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-trf==3.7.3) (0.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.26.4)\n",
            "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (0.1.1)\n",
            "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (0.0.9)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.5.0+cu121)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.10/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.9.11)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒  Use each of the three models that were downloaded above and perform named entitiy recognition with each of them on the original not preprocessed article, one after another. You can use different code cells for the different models or write everything into one cell, as you prefer. For each of the model outputs, automatically calculate the number of NERs for each NER type that the model identifies."
      ],
      "metadata": {
        "id": "oRt0fV4tbEXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here I use nlp.sm:\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#nlp_lg = spacy.load(\"en_core_web_lg\")\n",
        "#nlp_trf = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "# I use the original not preprocessed article:\n",
        "doc = nlp(article.text)\n",
        "\n",
        "# this will print the named entities, their starting and ending character index of the entity within the document, and their NER type:\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "# Calculate the number of NERs for each NER type that the model identifies:\n",
        "entity_counts = Counter([ent.label_ for ent in doc.ents])\n",
        "print()\n",
        "print(entity_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NftYBdc8CbJz",
        "outputId": "53577390-b9c2-4f0e-8365-fa96546482ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN 0 3 ORG\n",
            "120,000 52 59 MONEY\n",
            "2019 63 67 DATE\n",
            "Maurizio Cattelan 156 173 PERSON\n",
            "Comedian 200 208 NORP\n",
            "Friday 248 254 DATE\n",
            "Sotheby’s 270 279 ORG\n",
            "three 316 321 CARDINAL\n",
            "$1 million to $1.5 million 387 413 MONEY\n",
            "one 489 492 CARDINAL\n",
            "Sotheby’s 593 602 ORG\n",
            "CNN 616 619 ORG\n",
            "Cattelan 841 849 NORP\n",
            "French 854 860 NORP\n",
            "Perrotin 873 881 PERSON\n",
            "five years ago 914 928 DATE\n",
            "Comedian 950 958 NORP\n",
            "six 967 970 CARDINAL\n",
            "the Art Basel Miami Beach 994 1019 FAC\n",
            "Miami 1078 1083 GPE\n",
            "CNN 1558 1561 ORG\n",
            "David Datuna 1618 1630 PERSON\n",
            "hundreds 1706 1714 CARDINAL\n",
            "Miami 1844 1849 GPE\n",
            "three 1923 1928 CARDINAL\n",
            "Two 1961 1964 CARDINAL\n",
            "120,000 2004 2011 MONEY\n",
            "third 2023 2028 ORDINAL\n",
            "Guggenheim 2108 2118 FAC\n",
            "New York 2129 2137 GPE\n",
            "Sotheby’s 2140 2149 ORG\n",
            "November 2197 2205 DATE\n",
            "one 2290 2293 CARDINAL\n",
            "Miami 2349 2354 GPE\n",
            "Cattelan 2369 2377 ORG\n",
            "Comedian 2393 2401 NORP\n",
            "the Art Newspaper 2440 2457 ORG\n",
            "2021 2461 2465 DATE\n",
            "Italian 2564 2571 NORP\n",
            "CNN 2677 2680 ORG\n",
            "November 2709 2717 DATE\n",
            "Comedian 2752 2760 NORP\n",
            "Sotheby's 2774 2783 ORG\n",
            "Sotheby’s 2826 2835 ORG\n",
            "Sotheby’s 2877 2886 ORG\n",
            "Americas 2920 2928 LOC\n",
            "David Galperin 2930 2944 PERSON\n",
            "Comedian 2957 2965 NORP\n",
            "Galperin 3125 3133 ORG\n",
            "Comedian 3166 3174 NORP\n",
            "first 3411 3416 ORDINAL\n",
            "Comedian 3423 3431 NORP\n",
            "the Leeum Museum of Art 3492 3515 ORG\n",
            "Seoul 3519 3524 GPE\n",
            "South Korea 3526 3537 GPE\n",
            "Seoul National University 3583 3608 ORG\n",
            "CNN 3777 3780 ORG\n",
            "2023 3791 3795 DATE\n",
            "Sotheby’s 3868 3877 ORG\n",
            "New York 3969 3977 GPE\n",
            "November 20 3994 4005 DATE\n",
            "Comedian 4008 4016 NORP\n",
            "Monday 4047 4053 DATE\n",
            "London 4100 4106 GPE\n",
            "Paris 4108 4113 GPE\n",
            "Milan 4115 4120 GPE\n",
            "Hong Kong 4122 4131 GPE\n",
            "Dubai 4133 4138 GPE\n",
            "Taipei 4140 4146 GPE\n",
            "Tokyo 4148 4153 GPE\n",
            "Los Angeles 4158 4169 GPE\n",
            "\n",
            "Counter({'ORG': 17, 'GPE': 15, 'NORP': 11, 'DATE': 9, 'CARDINAL': 7, 'PERSON': 4, 'MONEY': 3, 'FAC': 2, 'ORDINAL': 2, 'LOC': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here I use nlp.lg:\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp_lg = spacy.load(\"en_core_web_lg\")\n",
        "#nlp_trf = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "# I use the original not preprocessed article:\n",
        "doc = nlp_lg(article.text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "# Calculate the number of NERs for each NER type that the model identifies:\n",
        "entity_counts = Counter([ent.label_ for ent in doc.ents])\n",
        "print()\n",
        "print(entity_counts)"
      ],
      "metadata": {
        "id": "qdKi-YHGClQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6583ea-7867-47f3-fa3e-0e0ad5e0d320"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN 0 3 ORG\n",
            "120,000 52 59 MONEY\n",
            "2019 63 67 DATE\n",
            "Maurizio Cattelan 156 173 PERSON\n",
            "Comedian 200 208 WORK_OF_ART\n",
            "Friday 248 254 DATE\n",
            "Sotheby’s 270 279 ORG\n",
            "three 316 321 CARDINAL\n",
            "$1 million to $1.5 million 387 413 MONEY\n",
            "one 489 492 CARDINAL\n",
            "Sotheby’s 593 602 ORG\n",
            "CNN 616 619 ORG\n",
            "Comedian’ 692 701 WORK_OF_ART\n",
            "Cattelan 841 849 PERSON\n",
            "French 854 860 NORP\n",
            "Perrotin 873 881 ORG\n",
            "five years ago 914 928 DATE\n",
            "Comedian 950 958 WORK_OF_ART\n",
            "six 967 970 CARDINAL\n",
            "the Art Basel Miami Beach 994 1019 FAC\n",
            "Miami 1078 1083 GPE\n",
            "Marcel Duchamp 1322 1336 PERSON\n",
            "CNN 1558 1561 ORG\n",
            "David Datuna 1618 1630 PERSON\n",
            "hundreds 1706 1714 CARDINAL\n",
            "Miami 1844 1849 GPE\n",
            "three 1923 1928 CARDINAL\n",
            "Two 1961 1964 CARDINAL\n",
            "120,000 2004 2011 MONEY\n",
            "third 2023 2028 ORDINAL\n",
            "Guggenheim 2108 2118 ORG\n",
            "New York 2129 2137 GPE\n",
            "Sotheby’s 2140 2149 ORG\n",
            "November 2197 2205 DATE\n",
            "one 2290 2293 CARDINAL\n",
            "Miami 2349 2354 GPE\n",
            "Cattelan 2369 2377 PERSON\n",
            "Comedian 2393 2401 WORK_OF_ART\n",
            "the Art Newspaper 2440 2457 ORG\n",
            "2021 2461 2465 DATE\n",
            "Italian 2564 2571 NORP\n",
            "CNN 2677 2680 ORG\n",
            "November 2709 2717 DATE\n",
            "Comedian 2752 2760 WORK_OF_ART\n",
            "Sotheby's 2774 2783 ORG\n",
            "Sotheby’s 2826 2835 ORG\n",
            "Sotheby’s 2877 2886 ORG\n",
            "Americas 2920 2928 LOC\n",
            "David Galperin 2930 2944 PERSON\n",
            "Comedian 2957 2965 WORK_OF_ART\n",
            "Galperin 3125 3133 PERSON\n",
            "first 3411 3416 ORDINAL\n",
            "Comedian 3423 3431 WORK_OF_ART\n",
            "the Leeum Museum of Art 3492 3515 ORG\n",
            "Seoul 3519 3524 GPE\n",
            "South Korea 3526 3537 GPE\n",
            "Seoul National University 3583 3608 ORG\n",
            "CNN 3777 3780 ORG\n",
            "2023 3791 3795 DATE\n",
            "Sotheby’s 3868 3877 ORG\n",
            "New York 3969 3977 GPE\n",
            "November 20 3994 4005 DATE\n",
            "Monday 4047 4053 DATE\n",
            "London 4100 4106 GPE\n",
            "Paris 4108 4113 GPE\n",
            "Milan 4115 4120 GPE\n",
            "Hong Kong 4122 4131 GPE\n",
            "Dubai 4133 4138 GPE\n",
            "Taipei 4140 4146 GPE\n",
            "Tokyo 4148 4153 GPE\n",
            "Los Angeles 4158 4169 GPE\n",
            "\n",
            "Counter({'ORG': 17, 'GPE': 15, 'DATE': 9, 'PERSON': 7, 'WORK_OF_ART': 7, 'CARDINAL': 7, 'MONEY': 3, 'NORP': 2, 'ORDINAL': 2, 'FAC': 1, 'LOC': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here I use nlp.trf:\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "#nlp_lg = spacy.load(\"en_core_web_lg\")\n",
        "nlp_trf = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "# I use the original not preprocessed article:\n",
        "doc = nlp_trf(article.text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "# Calculate the number of NERs for each NER type that the model identifies:\n",
        "entity_counts = Counter([ent.label_ for ent in doc.ents])\n",
        "print()\n",
        "print(entity_counts)"
      ],
      "metadata": {
        "id": "ZpLgXnJxbMCf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "adfb2799-f80a-414e-aa71-2673797f3896"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E002] Can't find factory for 'curated_transformer' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, en.lemmatizer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-aacdeaf2c605>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#nlp = spacy.load(\"en_core_web_sm\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#nlp_lg = spacy.load(\"en_core_web_lg\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnlp_trf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_trf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# I use the original not preprocessed article:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"blank:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \"\"\"\n\u001b[1;32m    500\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/en_core_web_trf/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m     return load_model_from_path(\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_overrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m     nlp = load_model_from_config(\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_config\u001b[0;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;31m# registry, including custom subclasses provided via entry points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0mlang_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lang\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m     nlp = lang_cls.from_config(\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[1;32m   1887\u001b[0m                     \u001b[0;31m# The pipe name (key in the config) here is the unique name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     \u001b[0;31m# of the component, not necessarily the factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1889\u001b[0;31m                     nlp.add_pipe(\n\u001b[0m\u001b[1;32m   1890\u001b[0m                         \u001b[0mfactory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m                         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipe_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    819\u001b[0m             )\n\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             pipe_component = self.create_pipe(\n\u001b[0m\u001b[1;32m    822\u001b[0m                 \u001b[0mfactory_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0mlang_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             )\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mpipe_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_factory_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;31m# This is unideal, but the alternative would mean you always need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E002] Can't find factory for 'curated_transformer' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, en.lemmatizer"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the following function to visualize the named entities in the text in order to facilitate the analysis."
      ],
      "metadata": {
        "id": "LDM8iLRNcdKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also visualize the detected named entities\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "3CWq0ejhceIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ Add your text of the analysis of differences between the three different models right here in the next text field."
      ],
      "metadata": {
        "id": "of4jzY-JdA0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Counter seems to give similar output for the 3 models. However, some differences can be seen in the NER types that the models group the named entities into. All of the models use the following types: ORG, GPE, DATE, PERSON, MONEY, CARDINAL, LOC, ORDINAL, NORP.\n",
        "However, there are some differences:\n",
        "- Only the models sm and lg use FAC, which stands for Facility. For example, \"Guggenheim\" is recognised by the model sm as Facility, whereas the lg model groups it into ORG (organisation) and the trf model recognises \"The Guggenheim Museum\" and groups it into ORG too. This makes me believe that the models lg and especially trf are more accurate (at least concluding from this specific case).\n",
        "- Only the models lg and the trf models use WORK_OF_ART, which is supposed to apply to titles or names of creative works, such as the title of the artwork in the article (\"Comedian\"). This is actually an interesting one because the sm model interprets \"Comedian\" as NORP whereas the lg and the trf models interpret it as WORK_OF_ART, which is more accurate!\n",
        "- The trf model is the only one out of these 3 models that uses not only the type DATE, but also the additional types EVENT and TIME. The 3 models extract the same 9 named entities for date. The trf model additionally extracts 1 named entity for TIME (\"02:42\") which might be useful if we are looking for specific information regarding time in the text. The usefulness of the type EVENT is visible when looking at the NER type output for the named entity \"Art Basel Miami Beach\" which is recognised by the models sm and lg as FAC and by trf as EVENT. The type EVENT (and hence the trf model) is more accurate in this case because it actually is a fair/an event.\n",
        "\n",
        "All in all, the trf model seems to be most accurate in the above executed NER."
      ],
      "metadata": {
        "id": "KwDuP5aDdCN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ Compare the analysis of the best performing spaCy model for NER on the article after it was preprocessed to the performance on the non-preprocessed article."
      ],
      "metadata": {
        "id": "7CqztuZBcrw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here I use nlp.trf on the preprocessed version:\n",
        "\n",
        "nlp_trf = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "text_preprocessed = ' '.join(unique_words_after_preprocessing_spaCy)  # Join list items with spaces because the spaCy nlp() function expects a single string as input, not a list!\n",
        "\n",
        "doc = nlp_trf(text_preprocessed)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "# Calculate the number of NERs for each NER type that the model identifies:\n",
        "entity_counts = Counter([ent.label_ for ent in doc.ents])\n",
        "print()\n",
        "print(entity_counts)"
      ],
      "metadata": {
        "id": "2cxBSOWmc-5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When I compare the NER output of (the in my opinion best performing spaCy model) trf on the article after it was preprocessed with the performance on the non-preprocessed article, I can say that the model recognises fewer named entities in the preprocessed article: 29 for the preprocessed article, 76 for the non-preprocessed article. The named entities that the model recognised on the preprocessed text seem to be a lot less accurate; they include quite random words in non-fitting NER types such as \"take\" in type PERSON.\n",
        "\n",
        "It does make sense that the NER performs worse on the preprocessed text because we made all words lowercase and this probably makes it harder for the model to recognise named entities because these is often case-sensitive, e.g. \"Apple\" the company and \"apple\" the fruit. So an important signal for the model is taken away. Moreover, we lemmatised all words which might change entity names in a way that they cannot be recognised anymore. Furthermore, numbers (type MONEY, type DATE, type TIME) are obviously not included in the NER of the preprocessed text because we had previously removed all numbers and punctuation."
      ],
      "metadata": {
        "id": "PKgkx06j84Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multilingual NER**\n",
        "In this exercise, the NER performance of spaCy in English is compared to another language of your choice."
      ],
      "metadata": {
        "id": "enZaik7ZdPYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ Go the [spaCy page](https://spacy.io/models) detailing the available models to identify supported languages on the left listed under the heading \"Trained Pipelines\". Select a language and model of your choice. Find an article in this language and parse it using the newspaper package."
      ],
      "metadata": {
        "id": "h8DFV273eDBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remember that you first need to load the model by replacing\n",
        "#\"en_core_web_sm\" with the name of your model\n",
        "\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "\n",
        "url = 'https://www.derstandard.at/story/3000000242971/arnold-schwarzenegger-waehlt-kamala-harris'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "# Loading the model:\n",
        "nlp = spacy.load(\"de_core_news_sm\")\n"
      ],
      "metadata": {
        "id": "o3NW0KRYeCPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ Perform NER on the selected article."
      ],
      "metadata": {
        "id": "-KaoiUwqeLSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(article.text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "entity_counts = Counter([ent.label_ for ent in doc.ents])\n",
        "print()\n",
        "print(entity_counts)"
      ],
      "metadata": {
        "id": "AeDyii4SeT3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ How well did the NER in the language of your choice work as compared to the overall performance of NER with spaCy in English?"
      ],
      "metadata": {
        "id": "GFeUoLxheXao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This NER model for this German text did not perform as well as the NER models for the previous tasks in English; perhaps I chose a model that is not as good yet.\n",
        "\n",
        "Overall, the performance is mixed because some parts of the extraction are correct and some parts are incorrect.\n",
        "\n",
        "The LOC type worked well. All locations were extracted correctly. However, the other types did not always work well. The PER type worked moderately well: Some persons were extracted correctly, but some were not extracted at all, such as \"Kamela Harris\", and some named entities in the type PER are incorrect, such as \"Hass\".\n",
        "\n",
        "Moreover, the model extracted some named entities that are not actually named entities (e.g., \"republikanische\").\n",
        "\n",
        "It certainly had trouble with more complex phrases (e.g., \"republikanische Ex-Gouverneur\", \"Z.B. Browser-AddOns wie Adblocker\").\n",
        "\n",
        "The MISC type is used a few times but mostly for non-relevant or incorrect entities such as \"Z.B. Browser-AddOns wie Adblocker\" or for entities that should belong to other types, such as \"STANDARD\" which should be in type ORG. This might indicate that the model did not know how to group them but that it still considered them important enough to extract them."
      ],
      "metadata": {
        "id": "BURacTomelzy"
      }
    }
  ]
}